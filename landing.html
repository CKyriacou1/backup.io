<!DOCTYPE HTML>
<!--
	Forty by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title> MSc Data Science - University of Essex Online </title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
				<!-- Note: The "styleN" class below should match that of the banner element. -->
					<header id="header" class="alt style2">
						<a href="index.html" class="logo"><strong>E-portfolio</strong> <span>by Konstantinos Kyriacou </span></a>
						<nav>
							<a href="#menu">Menu</a>
						</nav>
					</header>

				<!-- Menu -->
					<nav id="menu">
						<ul class="links">
							<li><a href="index.html">Summary of Content</a></li>
							<li><a href="landing.html">MSc Data Science</a></li>
							<li><a href="generic.html">About Me</a></li>
							<li><a href="elements.html">Background and Skills</a></li>
						</ul>
						<ul class="actions stacked">
							<li><a href="#" class="button primary fit">Get Started</a></li>
							<li><a href="#" class="button fit">Log In</a></li>
						</ul>
					</nav>

				<!-- Banner -->
				<!-- Note: The "styleN" class below should match that of the header element. -->
					<section id="banner" class="style2">
						<div class="inner">
							<span class="image">
								<img src="images/pic07.jpg" alt="" />
							</span>
							<header class="major">
								<h1>Module Assignments</h1>
							</header>
							<div class="content">
								<p> I have culmilated a series of projects and assignents from my enactment within my master's degree.</p>
								<p> These assignments have been attributed with regards to the module they belong in.<p/>
							</div>
						</div>
					</section>

				<!-- Main -->
					<div id="main">

						<!-- One -->
							<section id="one">
								<div class="inner">
									<header class="major">
										<h2>Module 1</h2>
										<h3> Introductory Module</h3>
										<h4> Unit 1</h4>
										<!-- Box -->
													<h3>Paragraph Introducing Myself</h3>
													<div class="Paragraph Introducing Myself">
														<p> My name is Konstantinos and I live in Cyprus, Nicosia . 
														    I look forward to beginning the data science program, 
													            as to obtain the necessary knowledge to work as a data scientist in the future. 
														    In present, the prospect of working in the data science field excites me, 
														    as it also intimidates my person. The combination of the use of programming
													            and statistics within this field is a lead incentive for my person 
														    to approach this field, as I truly appreciates these subjects. <p/>
													</div>
										
										
										<h4> Unit 2</h4>
										<!-- Box -->
													<h3>The Importance of a Postgraduate Degree in the Computer Science Field</h3>
													<div class="The Importance of a Postgraduate Degree in the Computer Science Field">
                                                                                          <p>As the current market shifts and alters in regards to the advancement of science and it’s fields, it is natural for masters applicants to shape an opinion 
                                                                                         regarding this change. As one of those applicants, I have chosen to focus on a data science degree as I’ve realized that the general field of computer science is a large component 
                                                                                         for the growth of our current market, and I wish to be a part of this development. My assignment covers investigation into this matter, as well as highlighting the reasoning of my personal choice.
                                                                                         In the academic paper of Vasant Dhar, ‘Data Science and Prediction’, various academic definitions are given. It is mentioned that science is the systemic approach to acquiring knowledge.
                                                                                         Subsequently, data science is defined by Vasant Dharin in his article of ‘Data Science and Prediction’, as the focus of this systemic approach involving data as well as various other variables. 
                                                                                         These variables are defined by the author as different fields of science, as well as practices such as the systematic study of data ,regarding its properties,organization and analysis 
                                                                                         ,translating it into reliable deductions. The author insists that data science is a field separate of the previously mentioned variables and as such sets a need in the market for employment for 
                                                                                         individuals that have academic or previous experience within this field specifically, different from the previously stated. As such, the term computer science can be defined as the application
                                                                                         of science within a computerized environment, using previously mentioned variables. It is stated in the article ‘data science and prediction’ by Vasant Dhar, that the correlation between 
                                                                                         these two sectors could be defined as the predictive accuracy used by computers. Consequently, it is natural to say that by using the information presented in the article, these two sectors are 
                                                                                         linked and specifically that one may encapsule the other as a subcategory within its own field. This article manages to formulate this connection as it states the core category to be computer science, 
                                                                                         as data science is stated to be the product of a surge of computer based collected data in the new age.</p> 

<p>The article used above, states that as databases expand, the constant need for education is apparent, as to handle most current matters regarding the field.
In agreement to the previous statement, there is a constant need for the educating and employment of data scientists to be able to supply with these demands. 
Regarding this, the market has influenced universities and various other establishments to employ methods to educate potential candidates for these positions.
Specifically speaking, throughout the previous article it is mentioned that the employment market for data scientists has increased within recent years
,as noted within the article ‘Bridging the demand and the offer in data science’ by Adam S.Z. Belloum, Spiros Koulouzis, Tomasz Wiktorski and Andrea Manieri.
With this in mind, I have formulated the decision of educating myself within this specific field as a means to acquiring a position in the data science field of employment.</p>

<p>Furthermore, in regards to this personal decision of studying within this field, an additional reason as to why I am currently seeking to pursue a masters 
degree in this field is stated in the ‘2015 data science salary survey’, written by King, J. and Magoulas, R. ,that relays the information 
that ‘the median annual base salary for a starting data scientist is $91,000’. This information relays to my person, that attaining a position within this 
field will provide me with financial security and potential for salary growth. Additionally, these figures according to the ‘2015 data science survey’ by King, J. and Magoulas, R.
,have been consistent over time. These elements were one of the defining factors in my decision to study data science as a master’s degree. Besides the previous stated fact, 
I chose this path as it offered a fixed set of hours in most employment positions, as referred to in the article by J.King and R.Magoulas, ‘2015 data science survey’. 
This allows me to experience other opportunities outside of my employment hours, as it provides me with some time for leisure activities. 
In agreement with my previous statement, I also chose this path, as a way to further explore my abilities in programming and statistics
,as well as my passion for science and learning. Lastly, It is important to note that the prospect of new academics in the field will resort in more accurate results 
regarding the handling of large databases and thus more accurate solutions for various issues, which is referred to in the article ‘Data Science and Prediction’. 
The result of a larger pool of specialists is the insurance of outcomes with greater accuracy.</p>
<p>In conclusion, I wish to be part of this unit of professionals, adding to the discovery and innovation within the data science field
,whilst allowing to possess all the benefits I have stated above.</p>


														
<h5>References:</h5>

<p>Aral, S. and Walker, D., 2012. Identifying influential and susceptible members of social networks. Science, 337(6092), pp.337-341.</p>

<p>Anderson, C., 2008. The end of theory: The data deluge makes the scientific method obsolete. Wired magazine, 16(7), pp.16-07.</p>
	
<p>Belloum, A.S., Koulouzis, S., Wiktorski, T. and Manieri, A., 2019. Bridging the demand and the offer in data science. Concurrency and Computation: Practice and Experience, 31(17), p.e5200.</p>
														
<p>Buchan, I., Winn, J. and Bishop, C., 2009. A unified modeling approach to data-intensive healthcare. In The fourth paradigm: data-intensive scientific discovery (pp. 91-98). Microsoft Research.</p>
	
<p>Dhar, V. and Chou, D., 2001. A comparison of nonlinear models for financial prediction. IEEE Transactions on Neural Networks, 12(4), pp.907-921.</p>

<p>Dhar, V., 2011. Prediction in financial markets: The case for small disjuncts. ACM Transactions on Intelligent Systems and Technology (TIST), 2(3), pp.1-22.</p>
	
<p>Dhar, V., 2013. Data science and prediction. Communications of the ACM, 56(12), pp.64-73.</p>
		
<p>King, J. and Magoulas, R., 2015. 2015 data science salary survey. O'Reilly Media, Incorporated.</p>
	
<p>Provost, F. and Fawcett, T. Data Science for Business. O'Reilly Media, New York, 2013.</p>

<p>Piatetsky-Shapiro, G., 1991. Discovery, analysis, and presentation of strong rules. Knowledge discovery in databases, pp.229-238.</p>



	
												</div>
										
										




									</header>
									
								</div>
							</section>

						<!-- Two -->
							<section id="two" class="spotlights">
								<section>
									<a href="generic.html" class="image">
										<img src="images/pic08.jpg" alt="" data-position="center center" />
									</a>
									<div class="content">
										<div class="inner">
											<header class="major">
												<h3>Module 2</h3>
											</header>
											<p>Nullam et orci eu lorem consequat tincidunt vivamus et sagittis magna sed nunc rhoncus condimentum sem. In efficitur ligula tate urna. Maecenas massa sed magna lacinia magna pellentesque lorem ipsum dolor. Nullam et orci eu lorem consequat tincidunt. Vivamus et sagittis tempus.</p>
											<ul class="actions">
												<li><a href="generic.html" class="button">Learn more</a></li>
											</ul>
										</div>
									</div>
								</section>
								<section>
									<a href="generic.html" class="image">
										<img src="images/pic09.jpg" alt="" data-position="top center" />
									</a>
									<div class="content">
										<div class="inner">
											<header class="major">
												<h3>Module 4</h3>
											</header>
											<h4>Unit 1 (Introduction to Big Data Technologies and Data Management):</h4>
											<p>Discussion Topic: Critically evaluate the rationale behind the Internet of Things (IOT), 
											in the context of the article by Huxley et al (2020), highlighting the opportunities, limitations, 
											risks and challenges associated with such a large-scale process of data collection.</p>
											
<p>During the procedure of analyzing large amounts of data, usually we face many difficulties and challenges.
Processing large data sets can be very time consuming in our effort to draw the right conclusions and results. In
addition to this, the data collection sometimes can be highly constrained or highly delayed. Furthermore,
circumstances that data is sent from low latency environments by thousands of devices, make it necessary to
process at a rapid pace the data. In fact, that is the reason why proper design for the analytical procedure is
necessary to effectively manage these situations.
Furthermore, most of the times processing large quantities of data can be very challenging regarding the cost
required. In detail, a high-performance processing can be very costly mainly due to the high-power bills. However,
although low performance processing can be a good way to conserve energy usually it doesn't correspond to the
demand of the big data analysis. Therefore, it is necessarily important to apply a new architecture model to satisfy
both sectors.
The processing of large amounts of data often comes up against limitations that are usually created by the
processing of personal data. We need to be sure that all data protection laws are respected during the data analysis
process. This can be extremely challenging regarding the processing of big data. Personal data must be provided
only for specified and legitimate purposes and not for further processing that does not correspond to these purposes.
Moreover, the analysis of large amounts of data is confronted with many risks. The sources of the data can often
be unreliable, giving us the wrong information. This has as effect in the extraction of results that do not correspond to
reality, thus producing incorrect data analysis and conclusions.
Besides, a proper management of big data give us the opportunity to progress in different sectors. This is because
accurate data analysis leads to accurate decision making. Due to this fact in many areas can be achieved
development, and lay a solid foundation for advancement in science, medicine, and business.</p>
		
<p>References:
1)Forgó, N., Hänold, S. and Schütze, B., 2017. The principle of purpose limitation and big data. In New technology,
big data and the law (pp. 17-42). Springer, Singapore.
2)Ji, C., Li, Y., Qiu, W., Jin, Y., Xu, Y., Awada, U., Li, K. and Qu, W., 2012. Big data processing: Big challenges and
opportunities. Journal of Interconnection Networks, 13(03n04), p.1250009.
3) Huxley, E 2020, Big data architectures, viewed 26 September 2022, <https://learn.microsoft.com/en-
us/azure/architecture/data-guide/big-datal>
4Krasnow Waterman, K. and Bruening, P.J., 2014. Big Data analytics: risks and responsibilities. International Data
Privacy Law, 4(2), pp.89-95.
5)Agrawal, D., Bernstein, P., Bertino, E., Davidson, S., Dayal, U., Franklin, M., Gehrke, J., Haas, L., Halevy, A., Han,
J. and Jagadish, H.V., 2011. Challenges and opportunities with Big Data 2011-1.</p>
	
	
Comments: This was the first individual work that enabled us to point out the opportunities, limitations, risks and challenges 
	  in relation with the data collection process. After anally examining the Lecturecast and other literature, most relevant to the topic
	 ,I managed to contribute to the discussion as follows. Following the completion of this task, I received an immense amount of information 
	  regarding the data collection process (opportunities, limitations, risks and challenges) which will present to be a very significant skill 
	  for my career afterwards, as a data scientist in which I will have to be capable of retrieving, handling large quantities of data and managing 
	  to take the correct data-driven decisions.
	
	
	
<h4>Unit 2 (Introduction to Data Types and Formats):</h4>
<p> Assignment: Considering what you have now learned in Units 1 and 2, you should respond 
                to at least three of your peers’ contributions from the Collaborative Discussion 1. 
	        To guide you, look at the guidelines for the peer review process on the Department’s page (Group/Teamwork).
	
	
<div class="col-4"><span class="image fit"><img src="images/207115317-d6840b89-b46a-43cf-9de5-f5a997868824-2.jpg" alt="" /></span></div>
<div class="col-4"><span class="image fit"><img src="images/207115317-d6840b89-b46a-43cf-9de5-f5a997868824.jpg" alt="" /></span></div>
<div class="col-4"><span class="image fit"><img src="images/207115336-ee6c6c43-bf7f-4a34-a4ce-b7cb1aff3164-2.jpg" alt="" /></span></div>
<div class="col-4"><span class="image fit"><img src="images/207115336-ee6c6c43-bf7f-4a34-a4ce-b7cb1aff3164.jpg" alt="" /></span></div>

	
	
<p> Comments: In exploring the obligations of my master’s degree, I was called to process information provided by my peers 
	      and submit specific notes as to promote their improvement in terms of the accuracy of their written work. 
	      Additionally, in relation to the promotion of the accuracy of their work, I inherently as a response to an assignment, 
	      formulated specific questions in order to promote the retrieval of missed information produced in their written work.
	      This specific occurrence served as the first time I attempted to interact with my peers in a written manner with previously
	      formulated content. It is important to mention that these individuals were chosen to be my teammates in a group project in 
	      the Units 6 and 11 within the module. Within the aforementioned process, I read the essays of my fellow peers, and after careful 
	      consideration of their work, I formulated replies in a manner that promoted the expression of my own thoughts as for the adjustment
	      of their content for their improvement of their written content, whilst also inherently producing questions as to allow for the 
	      clarification of certain aspects unexplored within their work.

<h4>Unit 3 (Data Collection and Storage):</h4>
											
<p>Assignment: In this third and final week of your first discussion, you should provide a summary post
	       based on your initial post, the feedback from your peers and the content of Units 1, 2 and 3. 
	       Please label this as ‘Summary Post’. It should be 300 words.</p>
											
<p>As our society and our knowledge evolve, so does the way we can use data. Nowadays, the decisions of
organizations and companies are fully guided by large masses of data which are imported from different data
sources ,in order to be processed and analyzed for this purpose. The completion of this procedure , can be achieved
much more efficiently since technology and loT (internet of things) are progressing rapidly.
However, even with the rapid evolution of technology sometimes the processing and analysis of big data can be
very time-consuming and costly. Additionally, while storing our data we need to be sure that our data sources are
reliable and will lead us to the right conclusions and decisions, otherwise erroneous decisions may lead to disastrous
consequences. Furthermore, it is required that the General Data Protection Regulations (GDPR) are fully respected
during the data collection, data storage and data analysis procedures. More precisely, data coming from various loT
devices, may contain some personal details which only certain people will be able to access. Missing and incomplete
data ,make the task of the specialist even more difficult ,since they have to keep in mind the appropriate
configuration of the data as well.
Therefore, it becomes necessary for a data scientist to take into consideration these challenges. Data needs to be
accessed timely, accurately, and efficiently. Data cleaning is a fundamental procedure which contributes in preparing
our database for processing and analysis. During this procedure, bad and irrelative data are being removed to
simplify our analysis process. At the same time, handling missing and incomplete data is a fundamental part of this
procedure which a data scientist should be aware.
Data is all around us in our everyday life. Hence, the correct harvesting and processing of data is of utmost
importance for modern companies. On the other hand, irresponsible data management can be disastrous and affect
the smooth operation of the business.</p>
											
<p>References:
1)Forgó, N., Hänold, S. and Schütze, B., 2017. The principle of purpose limitation and big data. In New technology,
big data and the law (pp. 17-42). Springer, Singapore.
2)Ji, C., Li, Y., Qiu, W., Jin, Y., Xu, Y., Awada, U., Li, K. and Qu, W., 2012. Big data processing: Big challenges and
opportunities. Journal of Interconnection Networks, 13(03n04), p.1250009
3) Huxley, E 2020, Big data architectures, viewed 26 September 2022, <https://learn.microsoft.com/en-
us/azure/architecture/data-quide/big-datal>.
4Krasnow Waterman, K. and Bruening, P.J., 2014. Big Data analytics: risks and responsibilities. International Data
Privacy Law, 4(2), pp.89-95.
5)Agrawal, D., Bernstein, P., Bertino, E., Davidson, S., Dayal, U., Franklin, M., Gehrke, J., Haas, L., Halevy, A., Han,
J. and Jagadish, H.V., 2011. Challenges and opportunities with Big Data 2011-1.</p>
	
Comments: Within the implications of enacting in unit 3, I was called to gather information from both units, one and two 
	  ,as to provide a summary post in regards to the posting of my initial post in unit 1 the feedback from my peers 
	  in unit 2 and lastly the implementation of the content provided in the previous and current units.
	  The objective of the specific assignment contributed to a well-rounded understanding of the matter at hand and 
	  as such gave me the material necessary to relay information about this topic.
											
	
<h4>Unit 4 (Completion of the Data Management Pipeline Test):</h4>

<div class="col-4"><span class="image fit"><img src="images/207070398-95677a71-21a3-4141-bbeb-6e3a2604f95e-2.png" alt="" /></span></div>
<div class="col-4"><span class="image fit"><img src="images/207070398-95677a71-21a3-4141-bbeb-6e3a2604f95e.png" alt="" /></span></div>
											
<p> Comments: This specific unit obligated my person to complete the data management pipeline test, which was automatically marked with 
	      its completion.This test enabled us to notice certain weaknesses in the knowledge obtained by the collective student unit, 
	      with regards to individual testing in the Data Management Pipeline. A Data Management Pipeline is a way of transporting 
	      data from one place (the source) to a destination (such as a data warehouse). During this procedure, the data is transformed 
	      and optimized so that it can be used to draw subsequent conclusions. </p>
											
<h4>More specifically the Data Management Pipeline consists of the following steps:</h4>
													<dl>
														<dt>1. Capturing raw data </dt>
														<dd>
															<p> The extraction of data from primary sources where data is kept or used. </p>
														</dd>
														<dt>2. Data cleaning </dt>
														<dd>
															<p> Checking data source, parsing, removing of duplicated and redundancies, data scraping, capture header information to enable logical and semantically relevant processing of data. </p>
														</dd>
														<dt> 3. Data integration </dt>
														<dd>
															<p> The process of gathering data from different sources and fit them together.</p>
														</dd>
														
														<dt> 4. Database design </dt>
														<dd>
															<p> It is the procedure of designing data models using a database management system. </p>
														</dd>
														
														<dt> 5. Data analysis </dt>
														<dd>
															<p>  The process of analyzing the data to reach to obtain the correct conclusions. </p>
														</dd>
														
														<dt> 6. Data Visualization </dt>
														<dd>
															<p> It is the procedure of representing data in a graphical form which is more understandable by the user. </p>
														</dd>
													</dl>



<h4>Unit 5 (Data Cleaning and Automating Data Collections):</h4>
<p> Comments: The implications of in acting with the materials given in unit 5, have been conditioned to be the practical application
	 of data cleaning, allocating python examples in accordance with data taken from household-level surveys carried out by UNICEF.
	 As per enacting within the 5th unit, it becomes apparent that the subject at hand is the practice of data cleaning. 
	 Data cleaning can be defined as the process by which we formulate our data in a dataset in such a way that we fix or remove each 
	 data that is incorrect, incomplete, remove duplicates and filter unwanted outliers as well. Additionally, this procedure ensures 
         that the dataset contains only accurate information. Therefore, this unit aided my person in expanding my knowledge in data cleaning 
         and comprehend how I can practically implement the data cleaning techniques using Python examples. </p>
											
											
											
											
<h4>Unit 6 (Database Design and Normalisation):</h4>
<h5> Instructions Concerning the Project: For the first part of your assessment, you are advised to position yourself and your team
	                                  as IT Software Consultants and Developers. You have been commissioned to design and build 
	                                  a single logical database. You get to choose the application environment (and hence the client profile),
	                                  and it is recommended that you choose an area of interest in a work environment or public service. 
	                                  This could be the postal service, retail industry/customer service, transport system, security services 
	                                  or the floral industry.</h5>
	
	                              <p> Once you have confirmed your design, you are required to develop the database and present an executive summary
					  to the senior management team, describing how it meets their requirements. Your team is expected to prepare and deliver 
					  a design report of your intended development work for the organisation. Please refer to the Lecturecast in this unit for guidance. 
					  Your design should capture the following. Please Note: The associated grading criteria are highlighted in the requirements below, 
					  to be reviewed alongside the Grading Criteria (in Module Resources).</p>
											
											
				       <h5> Grading Criteria: <h5>
				       <dl>
														<dt>Logical design</dt>
					                                                                        <dd>
															<p> Data items/entities, attributes of the data items chosen, relationships and associations. Identify and explain the data types used and data formats selected.</p>
														 
														</dd>
														<dt> Evaluation</dt>
														<dd>
															<p> Critically evaluate the data management pipeline process with regards to discussing the capturing of the data used and detailing its source, documenting how you implemented data cleaning techniques and the stages that have been carried out during the cleaning process. </p>
														</dd>
														<dt> Produce a proposal of the database build, creating an intended database model design.  </dt>
														<dd>
															<p>You should propose a database management system that you will be using for the build, taking into account the requirements of storage, user access, and the manipulation and retrieval of data within the proposed database. </p>
														</dd>
														
														<dt> Presentation and Structure </dt>
														<dd>
															<p> Presentation and Structure of your work (weighted at 25%) includes spelling, style, evidence of proofreading, correct use (and format) of citations and references. </p>
														</dd>
														
														
													</dl>
					       
					       

<div class="col-4"><span class="image fit"><img src="images/207074963-4406fc49-845a-4f87-a2ad-0f217d8e7a87-2.jpg" alt="" /></span></div>
<div class="col-4"><span class="image fit"><img src="207074985-049da056-1742-4c36-ad2d-0650031c8ee4-2.jpg" alt="" /></span></div>
<div class="col-4"><span class="image fit"><img src="207074994-2e350a76-0b16-491f-8faa-291ae6ab48f1-2.jpg" alt="" /></span></div>
<div class="col-4"><span class="image fit"><img src="207074994-2e350a76-0b16-491f-8faa-291ae6ab48f1.jpg" alt="" /></span></div>

<h5>References:</h5>

<p> 
1. AltexSoft, (n.d.). What is Data Pipeline: Components, Types, and Use Cases. [online]
Available
at:
https://www.altexsoft.com/blog/data-pipeline-components-and-types/.
[Accessed 29 October 2022].
2.
Krishnan, S. and Wu, E., 2019. Alphaclean: Automatic generation of data cleaning
pipelines. arXix preprint arXiv:1904.11827.
3. Llwer Newydd (2021) The Wales Transport Strategy 2021. Available from:
https://gov.wales/lwybr-newydd-wales-transport-strategy-2021 [Accessed 27 October
2022].
4. Neutatz, F., Chen, B., Abedian, Z. and Wu, E., 2021. From Cleaning before ML to Cleaning
for ML. IEEE Data Eng. Bull., 44(1), pp.24-41.
5.
Salloum, S., Huang, J.Z. & He, Y., (2019). Exploring and cleaning big data with random
sample data blocks. Journal of Big Data 6(45). Available at:
https://doi.org/10.1186/s40537-019-0205-4. Accessed: [30 October 2022].
6. IBM. (2021) Uses of XML. Available from:
https://www.ibm.com/docs/en/i/7.2?topic=introduction-uses-xml [Accessed 31 October
2022].
7. IONOS. (2020) ISO 8601 - Effectively Communicate Dates and Times Internationally.
Available from: https://www.ionos.com/digitalquide/websites/web-development/iso-86011
[Accessed 31 October 2022].
8. Date, C. J. (2019) Database Design and Relational Theory: Normal Forms and All that
Jazz. California: apress. Available from: https://tinyurl.com/25epdfk7 [Accessed 31
October 20221.
9.
StudyTonight (no date) Third Normal Form (3NF). Available from:
https://www.studytonight.com/dbms/third-normal-form.php [Accessed 31 October 2022]
					       </p>
					       
<h5>Comments:</h5> 
<p> In this unit my person had the opportunity to be allocated in a unit regarding the project, 
allowing for my person to cooperate with Panagiotis and Tsitsi as to achieve the execution of this project. 
The project topic was to design and build a single logical database in an area of our preference. 
My responsibility was the first part of the project in which I manage to create the logical design 
of the database (data items/entities, attributes of the data items chosen, relationships and associations).
Apart from the knowledge gained regarding the project’s subject, the entity of my team was able to develop skills 
such as cooperation, communication, and organization. These values are regarded as the epitome of a well-trained 
and professional individual within the workplace, as such it is applicable to mention that these skills aid an 
individual to achieve success within a unit in the workplace. </p>
	
					       
					       					       
					     
<h4> Feedback: </h4>
<p> 
The report offers a good demonstration of knowledge and understanding of relevant topics. The sections further 
include very goodunderstanding in the relevant key areas of knowledge. The logical database design includes purpose
specifications and entity relationship definitions. The data managemnet pipeline discusses data sources,
processing, and destination. The database design normal forms are briefly indicated. The reference section
includes nine references which are cited to support arguments in various sections. Application of knowledge
and understanding The project redresses a practical case of designing a database for an integrated ticketing
system commissioned by a transport authority. Theoretical descriptions are linked to the practical
requirements as communicated by an authority's strategy document. The database design section could
have been more elaborate and meaningful. Criticality There was a satisfactory demonstration of critical
analysis with regards to linking theory and practice. This is particularly apparent in the section on data
management pipeline. The discussion could have been more focused, for example the data sources being
more specific on information systems access. Structure and Presentation (as detailed in the assessment
guidance) The word count is around 1000 +10% (excluding TOC and references), and hence within the limit.
The report structure follows the requirements as it includes three sections on logical design, critical
evaluation of the data pipeline, and database proposal. Although not affecting the word count, the ERD
diagram is included twice with the latter (in the data design section) being redundant and could have been
replaced with more elaborate explanations. The conclusions section is very short and seems to have been
included just for the sake of having a conclusion section.</p>
					       
<h4>Unit 7 (Constructing Normalised Tables and Database Build):</h4>
<div class="col-4"><span class="image fit"><img src="images/207074343-44966a5d-6747-45c2-8524-fff75823c89a-2.jpg" alt="" /></span></div>
<div class="col-4"><span class="image fit"><img src="207073926-4ee9f523-c3bf-44e9-a59e-2520c558d653-2.jpg" alt="" /></span></div>
<div class="col-4"><span class="image fit"><img src="207074147-127a0ed9-a85c-475c-b827-4f31e76fac09-3.jpg" alt="" /></span></div>
<div class="col-4"><span class="image fit"><img src="207074234-92d69ca8-ad4c-4ccd-b164-33989ff63e6e.jpg" alt="" /></span></div>
					       
<h5>Comments:</h5>
<p>
In this unit we focused on constructing normalized tables and database build. 
More specifically, my person was given a table in un-normalized form, and after 
following specific steps which are shown in detail in the document, I managed to 
comply the table with the 3rd Normal Form (3 NF). In continuance with the completion 
of the first part of the task, I was able to proceed to the next process allocated 
within a series of procedures I needed to complete. This step was allocated to build 
a relational database system, with linked tables, enacting in the application of primary 
and foreign keys.This unit was imperative to my person gaining knowledge on certain 
procedures regarding normalized tables and database build. The integration of certain 
rules applying to the creation of normalized tables can be allocated below. This procedure 
is intendent to prevent anomalies(unexpected errors of the data ),as such insertion anomalies, 
deletion anomalies and modification anomalies. Additionally, it must be noted that the 
differentiation of both keys lies within the differentiation of their function. 
Primary key is one or more columns which offer uniqueness for every individual record in a 
table while a foreign key is a column in a table which refers to a column in a different 
table ensuring referential integrity.
	

<h3>Blockquote</h3>
<blockquote>
Specifically,a table which applies in 1st Normal Form (1 NF),2nd Normal Form (2 NF) and 3rd Normal Form(3 NF) 
should follow the specific objectives, respectively:

	
	
	
First Normal Form(1 NF):

-All rows must be unique (no duplicate rows)

-Each cell must only contain a single value (not a list)

-Each value should be non dividible(can’t be split out further)

	
	
	
	
Second Normal Form (2 NF):

-Follow the First Normal Form (1 NF) and

-All attributes (non-key columns) dependent on the key (no partial-dependency)
	
	
	
	
	
Third Normal Form (3 NF):

-Follow the Second Normal Form (2 NF) and

-Remove transitive dependencies between non-primary key columns.
</blockquote>

					       
					       
					       
					       

<h4>Unit 8 (Compliance and Regulatory Framework for Managing Data):</h4>
 <p> Within the contect of an assignment bestowed to us, the <h5> Collaborative Discussion 2 </h5>, 
we were called to compare compliance laws, in regards to specific paramenters.

 <p> The parameters are these: In relation to the securing of personal data rule, with either similar compliance laws within your country of residence, or with the ICO in the UK.

It is important to note that ICO refers to this rule as ‘Security’ and you should discuss your findings in relation to the standards set out and the exemptions that exist:

‘The securing personal data principle of the GDPR: Personal data shall be processed in a manner that ensures appropriate security of the personal data…’ (ICO.org.uk). </p>


<h4> Answer to Assignment: Comparing Compliance Laws </h4>
<p>
GDPR are laws that restrict the freedom of companies in relation to how they can use and collect the personal data
of individuals. Therefore, firms must be able to reasonably foresee everything they do with the data, and accurately
apply the law ,since otherwise they may be imposed with large fines. Similarities and differences can be
distinguished in regards with the application of these principles between countries.
To be more precise, the equivalent GDPR applied in the UK is called UK GDPR, which is almost identical with the
EU GDP. In general, the core definitions and legal terminology as such personal data, rights of data subjects,
controller and processor remain the same. However, in their application, these two regulations, present some slight
differences. For instance, a remarkable difference is that the age of consent at which an individual is able to provide
his personal data is 13, unlike in the EU which is 16. In addition, UK GDPR, appears to focus particular emphasis on
sectors such as national security, intelligence services and immigrations, while these regions are outside of the
scope of the EU GDPR. The provisions of the EU GDPR have been incorporated directly into UK law as the UK
subsequent version of the GDPR, affirming the same security regulations with compliance provided to different
countries. As per the implications of transactions occurring within UK organizations in regard to transactions
transpiring with EU residents it is adherent that the processing of such domestic personal data, whilst offering goods
and services to, or monitoring the behavior of EU residents, is attributed to follow the guidelines of the EU GDPR.
Lastly and more specifically, security measure applied within the two countries are proven to be equivalent and are
applied under different authoritative bodies (UK and EU).
Therefore, at a time when the development of technology is becoming more apparent, the capacity of
organizations to encounter personal data is becoming easier. That makes the compliance with all General Data
Protection Regulations (GDPR) even more crucial. </p>
	
<p> 
References:
1)www.cookiebot.com. (2021). UK-GDPR: new UK data law after Brexit | Compliance with Cookiebot. [online]
Available at: https://www.cookiebot.com/en/uk-gdpr/.
2) www.gdpreu.org. (2022). Differences between the UK-GDPR and the EU GDPR regulation. [online] Available
at: https://www.gdpreu.org/differences-between-the-uk-and-eu-gdpr-regulations [Accessed 24 Nov. 2022].
3) Lexology. (2021). GDPR vs. UK-GDPR; the laws Post Brexit. [online]
Available at:
https://www.lexology.com/library/detail.aspx?g=430eb8f4-8d52-4229-be9e-bb1b0f74fd56 [Accessed 24 Nov. 2022] </p>
					       

<h5>Comments: </h5>
<p>
The collaborative discussion allocated in Unit 8 required of the individuals in attendance of the module to 
compare the rules of EU GDPR vs UK GDPR. The implications of enacting within the exercises provided in unit 8 
allowed for the learning of new knowledge. Generally, in enacting within the material given within this unit 
my person was able to understand specific regulations regarding data and it’s use in certain areas. Generally,
the GDPR are legal regulations in relation to the data protection and privacy, so their application is crucial 
for each organization which handles data. Therefore, as a future data scientist who will work on a daily basis 
with data, the detailed knowledge of the principles is necessary. </p>
					       
					       
					       



<h3>Unit 9 (Database Management Systems (DBMS) and Models):</h3>
<h5> Within this module, I enacted in preparation for my seminars: </h5>
													<div class="row">
														<div class="col-6 col-12-small">

															<h4>Most Python codes and datasets are provided in files included in the Kazil textbook’s repository (see Unit 2 and 4 Reading). 
																The main aim here is therefore not learning the Python programming language, but rather learning how Python may be used
																in data analysis.Complete an example with storing data in a relational database. The example uses SQLite and includes the 
																following parts: </h4>
</h4>
															<ul>
																<li>Installing SQLite and setting a relational database with Python: See the section Setting Up Your Local Database with 
																Python on pages145-146 of the Kazil textbook. You are also able to use the SQL workspace in Codio.</li>
																<li>Saving the cleaned UNICEF dataset into the SQLite database: See pages 193-194 and refer to Unit 4 if necessary. </li>
																
															</ul>

															<h4>In relation to the new knowledge obtained, the Unit 9 was mainly conjured to involve the Database Management Systems (DBMS) and 
															study of data models. The DBMS are software systems which are used as a connection between an end-user and a database. Therefore, 
															DBMS give us the opportunity to interact with databases, including creating, reading, updating and deleting data in the database). 
															In addition to this, database models, is the way the database is implemented. They are divided into three categories:</h4>
															<ul class="alt">
																<li>Hierarchical model (simpler relationship, easy to handle).</li>
																<li>The network approach (more flexible, difficult to develop due to its complexity of the relationships among entities).</li>
																<li>The relational model (the most usual data model, useful for connecting related tables and viewing the data from different angles). 
																Subsequently, during the Unit 9 seminar preparation my person moved on to the more practical side of this topic, which is allocated
																as building a DBMS. In fact, I managed to install SQLite, which is defined as a programming language which is used for extracting 
																data from a database. In turn, I produced a relational database. This database is based on the relational model of data adhering 
																to Python programming. Lastly, I saved the cleaned UNICEF dataset into the SQLite database..</li>
															</ul>

https://user-images.githubusercontent.com/101480754/208127987-07e6cfeb-d98b-4978-b5fe-1608fe7862b1.mp4

https://user-images.githubusercontent.com/101480754/208128021-20b83aa9-9a45-41d2-8e09-5cce2d3bc922.mp4

https://user-images.githubusercontent.com/101480754/208128054-d6a277d5-9f13-4af0-876a-53c66ffe1613.mp4

https://user-images.githubusercontent.com/101480754/208128082-1fe8f207-1cbb-42d8-b26b-29dd639f8313.mp4

https://user-images.githubusercontent.com/101480754/208128160-f8675624-d0f7-43cc-abe7-7d9f62e3b42c.mp4

(CODING!!!!!)
														
														
														
														<h3> Unit 10 (More on APIs (Application Programming Interfaces) for Data Parsing): </h3>
														<h4> Assignment Details: As a team, evaluate the security requirements of an API of your choice and write a
														     brief security requirements specification which mitigates against any risks associated with the API for enabling
														     data sharing, scraping and connectivity between a program code written in Python and any of the following file 
														     formats/management systems (XML, JSON and SQL). </h4>
														
														
														<div class="col-4"><span class="image fit"><img src="207065712-0ef17a16-0da7-4dee-a70b-962330d3a4d7.jpg" alt="" /></span></div>
														
														<h4>
															Comments: In this unit we focused on Application Programming Interfaces (APIs). Essentially, API stands as the 
															          mediator of which two programs interconnect with each other. More specifically, as to define the 
															          acronyms standing within the name, it becomes apparent that the acronym of application refers to any 
															          software with a particular function and in contrast to this the acronym interface can be considered 
														                  as the connecting link between two applications. Lastly it is important to mention the acronym for 
															          the word programming alluding to the necessity of coding within this interactive system. Therefore, 
															          APIs are crucial in the normal function of applications, as they obtain data shared amongst other 
															          applications and execute predefined processes as to serve towards efficiency in both time and effort 
															          for the creator and for better calibration of many applications.The final part of executing the unit’s 
															          objectives was the execution of an assignment of which I was required to assess the security requirements 
															          of an API of my preference. Firstly, becoming acquainted with the security requirements and then following 
															          this action, providing certain precautionary measures to limit the risk of API as a way for data sharing, 
															          scraping and connectivity. </h4>
														
														
														
		
														
														
														<h3> Unit 11 (DBMS Transaction and Recovery): </h3>
														<h4> Details Regarding the Assignment:</h4> 
														<p> 
														The second part of the assessment is an Executive Summary of the completed design and build of a logical
															database based on the project report your team submitted at the end of Unit 6. Ensure you take account of the feedback
															you have received for the Unit 6 submission. You are required to produce an Executive Summary that pulls together your 
															findings, recommendations and conclusions in a clear and unambiguous format. Your report should also review concepts 
															underlying database modelling and critically evaluate the strengths and weaknesses of the relevant data models used in 
															your design and build. Your recommendations should take into account the legal and compliance requirements applicable 
															to the organisation. You will be submitting a report that informs your client of the outcomes of your analysis and 
															evaluation of your choice of data model highlighting strengths and limitations. </p>
														
														<div class="col-4"><span class="image fit"><img src="207077465-2694f8e3-13a2-4b91-a8d9-49d5d7c5deaa.jpg" alt="" /></span></div>
														<div class="col-4"><span class="image fit"><img src="207077490-17cae965-e406-48d6-9c10-fb44e7f24fe5.jpg" alt="" /></span></div>														
														<div class="col-4"><span class="image fit"><img src="207077505-5a8e9338-3b29-4884-8f28-fa77905ac176.jpg" alt="" /></span></div>														


														<h4> Comments: </h4>
														<p> 
															In Unit 11 we were required to submit an assessment. This assessment was produced, as per the finalization part of the 
															collective assessment performed in unit 6. As per this task, we were required to complete an executive summary regarding 
															the specific topic of the assignment within unit 6. The executive summary included our findings, recommendations, 
															conclusions, list of pros and cons of the relevant data models used in the submission in Unit 6. My person was responsible 
															for the part of the assignment which had to evaluate the strengths and weaknesses of the relevant data models used in our 
															design and build in Unit 6, and the creation of the renewed Entity-Relationship-Diagram based on our recommendations.
															The outcome of the second part of the project, which was allocated as the executive summary mentioned above, was much better 
															in terms of cooperation, since I was more familiarized with my teammates. We were able to operate as a team in a greater ease
															due to the familiarity between us and we were also able to conduct tasks with greater organization. The feeling of cooperation 
															and supportiveness was increased and as a result we encouraged each other to produce a better outcome and perform better on 
															the tasks at hand. These facts, in combination with the consideration of the feedback produced by the authoritative figure 
															minding our progress in Unit 6, allowed for the production of greater efficiency within the team. In addition to this, I 
															had the opportunity to notice the importance of values such as communication, cooperation and organization and better 
															instill them in my life as to support my everyday objectives in both a professional and personal level. </p>
														
														<h2>Reflections Regarding The Module</h2>
														<h3>
															In remembrance of the process I faced during my personal enactment within the obligations of this module, 
															I can recount mostly positive experiences and knowledge earned. This occurred as per my personal enactment
															within group assignments and individual exercises that procured my personal development and improvement 
															of my skills. More specifically, during the module, we were able to enhance our knowledge in regard to 
															the different data extraction, exploration, cleaning, and modelling techniques, which undoubtedly is of 
															crucial importance as a future data scientist. Furthermore, in relevance to this, a more general objective 
															maintained regarding this topic, was obtaining a general idea on this topic. </h3>
	
														<h3>
															In regards to this, the collective student unit had the opportunity to get familiarized with the 
															Data Management Pipeline procedure. As to better define this procedure, it is the process of transporting 
															data from one original location (the source) to a destination (such as a data warehouse). During this process,
															the data is transformed and optimized so that it can be used to draw subsequent conclusions. 
															Adherently there are steps that belong to this procedure, which can be allocated as the capturing of raw data,
															data cleaning, data integration, database design, data analysis and data visualization. 
															As previously mentioned, during the module we had the opportunity to enhance our skills in regards with 
															the data cleaning procedure. Data cleaning can be defined as the process, by which we formulate our data 
															in a dataset in such a way that we fix or remove each data that is incorrect, 
															incomplete, remove duplicates and filter unwanted outliers as well. Therefore, it ensures that the dataset 
															contains only accurate information. In addition to this, we had the opportunity to become familiarized with 
															the Database Management Systems (DBMS) and study of data models. The DBMS are software systems which are 
															used as a connection between an end-user and a database. Therefore, DBMS grants us with the ability to 
															interact with databases, including creating, reading, updating, and deleting data in the database. 
															In contrast, database model, is the way the database is implemented, and they are divided into three 
															distinct categories which are mentioned to be Hierarchical model, the network approach and the relational 
															model which is the most common of the three data models stated previously. </h3>
														
														
														<h3>
															As such, it is adherent to say that a data scientist with a great understanding of knowledge 
															adherent to his field must adapt to handling data and using this information and tools as he sees fit. 

															In relevance to activities being procreated by team efforts, there were many objectives referring to this effort. 
															In accordance with this notion, the use of cooperation, adherence to the opinions of others after debate 
															and the application of a verified answer on all parties allowed for the team to progress in a joint and agreed 
															progression. </h3>
														
														
														<h3>											
															In addition to this, amongst the many objectives appointed to my team during the enactment of our persons in this 
															module, the necessity for participation in group projects and assignments was prevalent as to produce 
															a better outcome and for the prevalence of the group’s effort. In adjacence to this, due to the 
															familiarization of the team members amongst each other, an environment was set as for the better 
															progression of joint efforts such as projects whilst also adhering to each person’s better abilities, 
															as to provide the adequate roles to each party as perceived amongst these noted abilities. This notion, 
															adhering to the greater outcome of the project due to the use of individuals towards their area of 
															expertise as to better set the prerequisites for a perfect scenario including time efficiency, 
															greater cooperation, and organization. As per the objectives that my team was obligated to complete,
															I can confirm that my personal enactment was vital to the completion of these previously mentioned 
															objectives. Specifically speaking, my enactment, for the production of the project my team was 
															responsible of producing, was of newfound experience for my person. The objective presented to our 
															team was the creation of a database system in accordance with a transportation system that was acting 
															in real time, as such on par with my individual efforts an additional team effort, entailed the 
															application of logical ideas produced by my team members and myself, to reenact interactions of everyday 
															users and project possible improvements for the application of better terms of use for certain items.
															In regard to the objectives performed within my involvement in assignments regarded towards individual
															and group enactment, I feel like I was able to achieve personal growth. </h3>
														<h3>
															As such I can inherently perceive my better understanding of certain practices involving the better cooperation 
															between teammates and additionally, I can further understand the magnitude of development procured within my person 
															as a return for my implications in the aforementioned processes and procedures. More specifically I can 
															notice that certain values have been ingrained in my person especially towards my enactment with 
															others in a team setting, such as cooperation and respect for the opinion of others. As well as 
															these values I have also noticed that my individual adopted certain traits. These traits can be defined
															as becoming a better listener enabling my person to adhere to certain objectives allowing for others 
															to involve themselves within a creative process that I am also involved. More specifically, a catalyst
															for my progressive change as an individual can be determined to be the objectives placed within this module
															and as such can be attributed more specifically to team projects.  </h3>
														<h3>
															Lastly, I desire to describe the overall experience during the module as successful, 
															since I have successfully captured the main objectives of the module and developed a greater understanding 
															towards the sector of data science. </h3>
														
														References:
														1) Admin (no date) Data extraction techniques and Tools, Rosoka. Rosoka Software. Available at: https://www.rosoka.com/blog/data-extraction-techniques#:~:text=In%20terms%20of%20Extraction%20Methods,Full%20Extraction%20and%20Incremental%20Extraction.&text=All%20data%20is%20extracted%20directly%20from%20the%20source%20system%20at%20once. (Accessed: December 12, 2022).
														2) Guide to data cleaning: Definition, benefits, components, and how to clean your data (no date) Tableau. Available at: https://www.tableau.com/learn/articles/what-is-data-cleaning (Accessed: December 12, 2022).
														3) Simplilearn (2022) What is data modelling? overview, basic concepts, and types in detail, Simplilearn.com. Simplilearn. Available at: https://www.simplilearn.com/what-is-data-modeling-article#:~:text=The%20following%20are%20the%20types,relationship%2C%20dimensional%2C%20and%20graph. (Accessed: December 12, 2022).
														4) What is a Data Pipeline & How Does It Work? (2022) Dremio. Available at:https://www.dremio.com/resources/guides/data-pipeline/ (Accessed: December 12, 2022).
														
												
											</ul>
										</div>
									</div>
								</section>
								<section>
									<a href="generic.html" class="image">
										<img src="images/pic10.jpg" alt="" data-position="25% 25%" />
									</a>
									<div class="content">
										<div class="inner">
											<header class="major">
												<h3>Module 5</h3>
											</header>
											<p>Nullam et orci eu lorem consequat tincidunt vivamus et sagittis magna sed nunc rhoncus condimentum sem. In efficitur ligula tate urna. Maecenas massa sed magna lacinia magna pellentesque lorem ipsum dolor. Nullam et orci eu lorem consequat tincidunt. Vivamus et sagittis tempus.</p>
											<ul class="actions">
												<li><a href="generic.html" class="button">Learn more</a></li>
											</ul>
										</div>
									</div>
								</section>
							</section>
						<section>
								
										

						<!-- Three -->
							
								</div>
							</section>

					</div>
						<section id="four">
								<div class="inner">
									<header class="major">
										<h2>Module 6:</h2>
									</header>
									<p>Discussion Topic: Critically evaluate the rationale behind the Internet of Things (IOT), in the context of the article by Huxley et al (2020), highlighting the opportunities, limitations, risks and challenges associated with such a large-scale process of data collection.</p>
								</div>
		</section>
		
						<section id="five">
								<div class="inner">
									<header class="major">
										<h2>Module 7</h2>
									</header>
									<p>Discussion Topic: Critically evaluate the rationale behind the Internet of Things (IOT), in the context of the article by Huxley et al (2020), highlighting the opportunities, limitations, risks and challenges associated with such a large-scale process of data collection.</p>
								</div>
							</section>
						
						
							</section>
						
		<section id="three">
								<div class="inner">
									<header class="major">
										<h2>      Author and Creator</h2>
									</header>
									<p>          I have enlisted my background and abilities I posses, within the coming section.</p>
									<ul class="actions">
										<li><a href="generic.html" class="button next">Learn more about the author</a></li>
									</ul>

				<!-- Contact -->
					<section id="contact">
						<div class="inner">
							<section>
								<form method="post" action="#">
									<div class="fields">
										<div class="field half">
											<label for="name">Name</label>
											<input type="text" name="name" id="name" />
										</div>
										<div class="field half">
											<label for="email">Email</label>
											<input type="text" name="email" id="email" />
										</div>
										<div class="field">
											<label for="message">Message</label>
											<textarea name="message" id="message" rows="6"></textarea>
										</div>
									</div>
									<ul class="actions">
										<li><input type="submit" value="Send Message" class="primary" /></li>
										<li><input type="reset" value="Clear" /></li>
									</ul>
								</form>
							</section>
							<section class="split">
								<section>
									<div class="contact-method">
										<span class="icon solid alt fa-envelope"></span>
										<h3>Email</h3>
										<a href="#">konstandinoskyriakou@gmail.com</a>
									</div>
								</section>
								<section>
									<div class="contact-method">
										<span class="icon solid alt fa-phone"></span>
										<h3>Contact</h3>
										<span>Unavailable</span>
									</div>
								</section>
								<section>
									<div class="contact-method">
										<span class="icon solid alt fa-home"></span>
										<h3>Address</h3>
										<span>Stationed in Cyprus <br />
										<br />
										</span>
									</div>
								</section>
							</section>
						</div>
					</section>

				<!-- Footer -->
					<footer id="footer">
						<div class="inner">
							<ul class="icons">
								<li><a href="https://ckyriacou1.github.io/" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
								<li><a href="https://www.linkedin.com/in/constantinos-kyriacou-datasciencemathstatistics/" class="icon brands alt fa-linkedin-in"><span class="label">LinkedIn</span></a></li>
							</ul>
							<ul class="copyright">
								<li>&copy; Konstantinos Kyriacou</li><li>Design: <a href="https://html5up.net">HTML5 UP</a></li>
							</ul>
						</div>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
